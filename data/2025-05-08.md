<div id=toc></div>

# Table of Contents

- [astro-ph.IM](#astro-ph.IM) [Total: 3]
- [astro-ph.EP](#astro-ph.EP) [Total: 4]
- [cs.LG](#cs.LG) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 2]


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [1] [Using anomaly detection to search for technosignatures in Breakthrough Listen observations](https://arxiv.org/abs/2505.03927)
*Snir Pardo,Dovi Poznanski,Steve Croft,Andrew P. V. Siemion,Matthew Lebofsky*

Main category: astro-ph.IM

TL;DR: A machine learning algorithm was used to search for extraterrestrial technosignatures in radio data from nearby stars, improving candidate selection but finding no confirmed signals.


<details>
  <summary>Details</summary>
Motivation: The exponential growth in radio observation data and the challenge of distinguishing potential extraterrestrial signals from human-made interference necessitate efficient analysis methods.

Method: Combined simulations and machine learning for anomaly detection, ranking candidates based on frequency uniqueness and temporal persistence.

Result: The method improved candidate quality, but no confirmed extraterrestrial signals were found after analyzing ~10^11 spectrograms.

Conclusion: Despite advanced techniques, no technosignatures were detected, highlighting the difficulty of the search.

Abstract: We implement a machine learning algorithm to search for extra-terrestrial
technosignatures in radio observations of several hundred nearby stars,
obtained with the Parkes and Green Bank Telescopes by the Breakthrough Listen
collaboration. Advances in detection technology have led to an exponential
growth in data, necessitating innovative and efficient analysis methods. This
problem is exacerbated by the large variety of possible forms an
extraterrestrial signal might take, and the size of the multidimensional
parameter space that must be searched. It is then made markedly worse by the
fact that our best guess at the properties of such a signal is that it might
resemble the signals emitted by human technology and communications, the main
(yet diverse) contaminant in radio observations. We address this challenge by
using a combination of simulations and machine learning methods for anomaly
detection. We rank candidates by how unusual they are in frequency, and how
persistent they are in time, by measuring the similarity between consecutive
spectrograms of the same star. We validate that our filters significantly
improve the quality of the candidates that are selected for human vetting when
compared to a random selection. Of the ~ 10^11 spectrograms that we analyzed,
we visually inspected thousands of the most promising spectrograms, and
thousands more for validation, about 20,000 in total, and report that no
candidate survived basic scrutiny.

</details>


### [2] [dfreproject: A Python package for astronomical reprojection](https://arxiv.org/abs/2505.03932)
*Carter Lee Rhea,Pieter Van Dokkum,Steven R. Janssens,Imad Pasham,Roberto Abraham,William P Bowman,Deborah Lokhorst,Seery Chen*

Main category: astro-ph.IM

TL;DR: A Python package, `dfreproject`, is introduced to speed up astronomical image reprojection by using GPU-optimized functions, achieving up to 20X speedups on GPUs and 10X on CPUs.


<details>
  <summary>Details</summary>
Motivation: Aligning sub-exposures in deep astronomical images is computationally expensive due to complex trigonometric transformations, creating bottlenecks in processing pipelines.

Method: The package uses Gnomonic projections for pixel-by-pixel shifts and interpolation tools to reproject images onto a common frame, adhering to FITS and SIP standards.

Result: `dfreproject` achieves significant speed improvements: 20X faster on GPUs and 10X on CPUs compared to alternatives.

Conclusion: The package efficiently addresses the computational bottleneck in astronomical image stacking, offering substantial performance gains.

Abstract: Deep astronomical images are often constructed by digitially stacking many
individual sub-exposures. Each sub-exposure is expected to show small
differences in the positions of stars and other objects in the field, due to
the movement of the celestial bodies, changes/imperfections in the
opto-mechanical imaging train, and other factors. To maximize image quality,
one must ensure that each sub-exposure is aligned to a common frame of
reference prior to stacking. This is done by reprojecting each exposure onto a
common target grid defined using a World Coordinate System (WCS) that is
defined by mapping the known angular positions of reference objects to their
observed spatial positions on each image. The transformations needed to
reproject images involve complicated trigonometric expressions which can be
slow to compute, so reprojection can be a major bottleneck in image processing
pipelines.
  To make astronomical reprojections faster to implement in pipelines, we have
written `dfreproject`, a Python package of GPU-optimized functions for this
purpose. The package's functions break down coordinate transformations using
Gnomonic projections to define pixel-by-pixel shifts from the source to the
target plane. The package also provides tools for interpolating a source image
onto a target plane with a single function call. This module follows the FITS
and SIP formats laid out in the seminal papers. Compared to common
alternatives, `dfreproject`'s routines result in speedups of up to 20X when run
on a GPU and 10X when run on a CPU.

</details>


### [3] [High polarization lines of the second solar spectrum of the Solar limb](https://arxiv.org/abs/2505.04239)
*Jean-Marie Malherbe*

Main category: astro-ph.IM

TL;DR: A dataset of high-resolution solar spectra for strongly polarized lines in the second solar spectrum, obtained using THEMIS telescope, is presented.


<details>
  <summary>Details</summary>
Motivation: To provide detailed spectra of the Sun's polarized lines for research on scattering polarization near the limb.

Method: Full Stokes polarimetry (I, Q/I, U/I, V/I) was conducted using the THEMIS telescope at various distances from the limb and disk center.

Result: Polarization rates up to 7% in CaI 4227 Å, 2% in SrI 4607 Å, and 1.4% in BaII 4554 Å were observed.

Conclusion: The dataset is made freely available in FITS format to support further research.

Abstract: We present a dataset of high resolution spectra of the Sun of many strongly
polarized lines belonging to the second solar spectrum, i.e. the spectrum near
the limb in linear polarization (scattering polarization). These solar spectra
were obtained in full Stokes polarimetry (I, Q/I, U/I, V/I) in the quiet Sun at
various distances from the limb, and at disk centre for comparison, with the
ground based CNRS THEMIS telescope. Polarization rates Q/I up to 7% are
obtained in CaI 4227 {\AA} line at $\mu$ = cos$\theta$ = 0, while 2% is reached
in SrI 4607 {\AA} line and 1.4% in BaII 4554 {\AA}. The spectra shown here are
freely available in FITS format to the research community.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [4] [Minimizing Star Spot Contamination of Exoplanet Transit Spectroscopy Using Alternate Normalization](https://arxiv.org/abs/2505.03881)
*Drake Deming,Miles H. Currie,Victoria S. Meadows,Sarah Peacock*

Main category: astro-ph.EP

TL;DR: The paper examines the impact of unocculted star spots on detecting molecules in exoplanet atmospheres, proposing an alternate normalization method to minimize their effects.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked issue of unocculted star spots in molecular detection simulations by Currie et al., focusing on their noise impact.

Method: Simulated detections of molecules (O₂, CO₂, CH₄, H₂O) via spectral cross-correlation, analyzing star spot effects and proposing an alternate normalization method.

Result: Star spots introduce significant noise, especially for water vapor. Molecular oxygen, carbon dioxide, and methane can still be detected without spot correction.

Conclusion: An alternate normalization method minimizes star spot effects, though practical challenges remain. A comprehensive approach is outlined to address these.

Abstract: Recently, Currie et al. simulated the detection of molecules in the
atmospheres of temperate rocky exoplanets transiting nearby M-dwarf stars. They
simulated detections via spectral cross-correlation applied to high resolution
optical and near-IR transit spectroscopy using the ELTs. Currie et al. did not
consider the effect of unocculted star spots, but we do that here for possible
detections of molecular oxygen, carbon dioxide, methane, and water vapor. We
find that confusion noise from unocculted star spots becomes significant for
large programs that stack tens to hundreds of transits to detect these
molecules. Noise from star spots increases with greater spot filling factors,
and star spot temperature has less effect than filling factor. Nevertheless,
molecular oxygen, carbon dioxide, and methane could be detected in temperate
rocky planets transiting nearby M-dwarfs without correcting for star spots.
Water vapor detections are the most affected, with star spots contaminating the
exoplanet signal as well as producing extra noise. Unocculted spots only affect
transit spectroscopy when normalizing by dividing by the total flux from the
star. We describe an alternate normalization method that minimizes star spot
effects by deriving and implementing an unspotted proxy spectrum for the
normalization. We show that the method works in principle using realistic
levels of random observational noise. Alternate normalization would be broadly
applicable to all types of transit spectroscopy, and we discuss challenges to
applying it in practice. We also outline a comprehensive approach that has the
potential to overcome those challenges.

</details>


### [5] [The Sonora Substellar Atmosphere Models. V: A Correction to the Disequilibrium Abundance of CO$_2$ for Sonora Elf Owl](https://arxiv.org/abs/2505.03994)
*Nicholas F. Wogan,James Mang,Natasha E. Batalha,Sagnick Mukherjee,Channon Visscher,Jonathan J. Fortney,Mark S. Marley,Caroline V. Morley*

Main category: astro-ph.EP

TL;DR: The paper introduces version two of the Sonora Elf Owl grid, correcting CO$_2$ quenching errors and removing PH$_3$ as a spectral contributor.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of substellar atmosphere models by addressing CO$_2$ quenching and PH$_3$ spectral issues.

Method: Updated the Sonora Elf Owl grid by correcting CO$_2$ quenching with respect to disequilibrium CO abundance and removing PH$_3$ contributions.

Result: Version two provides corrected CO$_2$ concentrations and improved spectra, addressing under-prediction and excessive PH$_3$ absorption.

Conclusion: The updated grid enhances the reliability of substellar atmosphere models for observational interpretation.

Abstract: To aid the interpretation of observations of substellar atmospheres,
Mukherjee et al. (2024) created the Sonora Elf Owl grid of model atmospheres,
simulations that accounted for disequilibrium quench chemistry. However, Sonora
Elf Owl did not accurately estimate CO$_2$ quenching because the models
quenched the gas with respect to the full atmosphere equilibrium, but CO$_2$
should have instead been quenched with respect to the disequilibrium (i.e.,
quenched) abundance of CO. As a result, Sonora Elf Owl under-predicted the
CO$_2$ abundance by several order of magnitude in some instances, an amount
that JWST is sensitive to. Here, we release version two of the Sonora Elf Owl
grid which has corrected CO$_2$ concentrations. Additionally, in version two we
remove PH$_3$ as a spectral contributor since our spectra consistently
contained too much PH$_3$ absorption. The new spectra can be found as an update
to the original Zenodo postings.

</details>


### [6] [Collisional Fragmentation Support in TRACE](https://arxiv.org/abs/2505.04399)
*Tiger Lu,Haniyeh Tajer,David M. Hernandez,Hanno Rein,Yurou Liu,Malena Rice*

Main category: astro-ph.EP

TL;DR: TRACE, a hybrid integrator in REBOUND, now supports collisional fragmentation and dynamic particle management, offering significant performance improvements (up to 70x speedup) compared to other integrators like MERCURIUS.


<details>
  <summary>Details</summary>
Motivation: To enhance TRACE's capabilities by enabling robust collision support, including fragmentation and dynamic particle handling, which was previously unsupported.

Method: Implemented back-end logic for collision support and tested TRACE's performance in a large-N protoplanetary disk simulation with various collision scenarios.

Result: TRACE matches the behavior of other integrators while achieving speedups of over 70x.

Conclusion: The updates make TRACE more versatile and efficient, now available in the latest REBOUND release.

Abstract: We present improved collision support for TRACE, a state-of-the-art hybrid
integrator in REBOUND. TRACE now supports collisional fragmentation and can
handle both removing and adding particles mid-timestep. We describe the
back-end logic implemented for robust collision support, and compare TRACE's
performance to other integrators including MERCURIUS on a large-N
protoplanetary disk simulation with various collision prescriptions, a system
which TRACE previously could not handle. TRACE matches the behavior of these
integrators, while offering potentially vast speedups of over 70x. All updates
described in this Note are available with the most recent public release of
REBOUND.

</details>


### [7] [A Detailed Investigation of HD 209458 b HST & JWST Transmission Spectra with SANSAR](https://arxiv.org/abs/2505.04413)
*Avinash Verma,Jayesh Goyal,Swaroop Avarsekar,Gaurav Shukla*

Main category: astro-ph.EP

TL;DR: The study investigates HD 209458 b's atmosphere using JWST and HST data with a new modeling framework, revealing discrepancies in metallicity and C/O constraints between instruments and methods.


<details>
  <summary>Details</summary>
Motivation: To resolve conflicting atmospheric constraints (e.g., metallicity, C/O) from previous observations of HD 209458 b using a new modeling approach.

Method: Uses SANSAR, a planetary atmosphere modeling framework, with free, equilibrium chemistry, and grid retrievals on JWST and HST data.

Result: Free retrievals show highly sub-solar metallicity and C/O, while equilibrium/grid retrievals align with solar values. NIRCam alone yields misleading constraints.

Conclusion: NIRCam observations alone can overestimate abundances; combining UV/optical and near-infrared data is essential for robust atmospheric constraints.

Abstract: HD 209458 b is the first exoplanet on which an atmosphere was detected. Since
then, its atmosphere has been investigated using multiple telescopes and
instruments. However, many of its atmospheric constraints remain debatable.
While HST observations suggested a highly sub-solar metallicity, recent JWST
NIRCam observations by Xue et al. 2024 constrained a super-solar metallicity
with highly sub-solar C/O. In this work, we show a detailed investigation of HD
209458 b transmission spectra observations from JWST and HST using SANSAR, a
newly developed planetary atmosphere modeling framework, with free, equilibrium
chemistry and self-consistent grid retrievals. The overall best-fitting model
with free retrievals ($\chi^2_{\rm{red}}$=1.21) constrains its metallicity and
C/O to be highly sub-solar, while equilibrium chemistry and grid retrievals
($\chi^2_{\rm{red}}$=1.27 and 1.30, respectively) are consistent with solar
values using STIS+WFC3+NIRCam observations. The retrieved abundances of H$_2$O
and CO$_2$ are almost three orders of magnitude lower (highly sub-solar) with
STIS+WFC3+NIRCam compared to just NIRCam, using free retrievals. NIRCam
observations alone also result in misleading constraints on metallicity and
C/O, with equilibrium chemistry and grid retrieval. We find that the model
choice of varying C/H or O/H to vary the C/O in equilibrium chemistry
retrievals leads to different metallicity constraints with NIRCam, but similar
constraints with STIS+WFC3+NIRCam. We conclude that NIRCam observations alone
can lead to overestimation of abundances for exoplanet atmospheres and,
therefore, should be used in combination with UV/Optical and near-infrared
observations to obtain robust constraints on abundances, C/O, and metallicity.
Specifically, even though we can detect the CO$_2$ feature with just NIRCam, we
cannot constrain its abundances robustly without the optical baseline.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Improved Dimensionality Reduction for Inverse Problems in Nuclear Fusion and High-Energy Astrophysics](https://arxiv.org/abs/2505.03849)
*Jonathan Gorard,Ammar Hakim,Hong Qin,Kyle Parfrey,Shantenu Jha*

Main category: cs.LG

TL;DR: The paper proposes a hybrid approach combining Monte Carlo sampling with formal verification to address high-dimensional inverse problems in nuclear fusion and astrophysics, ensuring physically and mathematically valid parameter space restrictions.


<details>
  <summary>Details</summary>
Motivation: High-dimensional inverse problems in fields like nuclear fusion and astrophysics involve large uncertainties and require extensive simulations. Existing methods like Monte Carlo sampling with dimensionality reduction lack guarantees of physical or mathematical validity.

Method: The paper suggests a hybrid approach integrating Monte Carlo sampling, non-linear dimensionality reduction (e.g., autoencoders), and formal verification methods to restrict parameter spaces while ensuring correctness.

Result: The proposed method aims to produce parameter space restrictions that are both mathematically and physically valid, accounting for experimental and model uncertainties.

Conclusion: The authors advocate for this hybrid approach to improve the reliability of solving high-dimensional inverse problems in fusion and astrophysics research.

Abstract: Many inverse problems in nuclear fusion and high-energy astrophysics
research, such as the optimization of tokamak reactor geometries or the
inference of black hole parameters from interferometric images, necessitate
high-dimensional parameter scans and large ensembles of simulations to be
performed. Such inverse problems typically involve large uncertainties, both in
the measurement parameters being inverted and in the underlying physics models
themselves. Monte Carlo sampling, when combined with modern non-linear
dimensionality reduction techniques such as autoencoders and manifold learning,
can be used to reduce the size of the parameter spaces considerably. However,
there is no guarantee that the resulting combinations of parameters will be
physically valid, or even mathematically consistent. In this position paper, we
advocate adopting a hybrid approach that leverages our recent advances in the
development of formal verification methods for numerical algorithms, with the
goal of constructing parameter space restrictions with provable mathematical
and physical correctness properties, whilst nevertheless respecting both
experimental uncertainties and uncertainties in the underlying physical
processes.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [9] [nuGAN: Generative Adversarial Emulator for Cosmic Web with Neutrinos](https://arxiv.org/abs/2505.03936)
*Neerav Kaushal,Elena Giusarma,Mauricio Reyes*

Main category: astro-ph.CO

TL;DR: A deep learning-based GAN model, νGAN, is proposed to emulate the Universe's cosmic webs for varying neutrino masses, achieving accurate results within 5% error on power spectrum for mildly non-linear scales.


<details>
  <summary>Details</summary>
Motivation: Understanding neutrino masses' impact on cosmic structure formation is vital for cosmology, but traditional simulations are computationally intensive.

Method: The study uses a generative adversarial network (GAN) called νGAN to generate 2D cosmic web maps for neutrino masses from 0.0 eV to 0.4 eV.

Result: νGAN produces statistically independent maps resembling true matter distribution, with 5% accuracy on power spectrum for mildly non-linear scales.

Conclusion: νGAN is a proof-of-concept for fast neutrino simulations, with potential for future extensions using higher-resolution 3D data and advanced models.

Abstract: Understanding the impact of neutrino masses on the evolution of Universe is a
crucial aspect of modern cosmology. Due to their large free streaming lengths,
neutrinos significantly influence the formation of cosmic structures at
non-linear scales. To maximize the information yield from current and future
galaxy surveys, it is essential to generate precise theoretical predictions of
structure formation. One approach to achieve this is by running large sets of
cosmological numerical simulations, which is a computationally intensive
process. In this study, we propose a deep learning-based generative adversarial
network (GAN) model to emulate the Universe for a variety of neutrino masses.
Our model called $\nu$GAN (for neutrino GAN) is able to generate 2D cosmic webs
of the Universe for a number of neutrino masses ranging from 0.0 eV to 0.4 eV.
The generated maps exhibit statistical independence, lack correlations with
training data, and very closely resemble the distribution of matter in true
maps. We assess the accuracy of our results both visually and through key
statistics used in cosmology and computer vision analyses. Our results indicate
that samples generated by $\nu$GAN are accurate within a 5% error on power
spectrum between k=0.01 to k=0.5 h/Mpc. Although this accuracy covers the
mildly non-linear scales, consistent with other works and observations,
achieving higher accuracy at fully non-linear scales requires more
sophisticated models, such as diffusion models. Nevertheless, our work opens up
new avenues for building emulators to generate fast and massive neutrino
simulations, potentially revolutionizing cosmological predictions and analyses.
This work serves as a proof-of-concept, paving the way for future extensions
with higher-resolution 3D data and advanced generative models.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [10] [Heartbeat Stars Recognition Based on Recurrent Neural Networks: Method and Validation](https://arxiv.org/abs/2505.04067)
*Min-Yu Li,Sheng-Bang Qian,Li-Ying Zhu,Wen-Ping Liao,Lin-Feng Chang,Er-Gang Zhao,Xiang-Dong Shi,Fu-Xing Li,Qi-Bin Sun,Ping Li*

Main category: astro-ph.SR

TL;DR: A novel feature extraction method using Fourier transform and RNNs achieves high accuracy in detecting heartbeat stars (HBSs) and identifies new candidates.


<details>
  <summary>Details</summary>
Motivation: Manual inspection of HBSs is inefficient; machine learning offers automated detection.

Method: Transform light curves to frequency domain, extract harmonics, normalize features, and train RNNs on synthetic data.

Result: 95% accuracy on synthetic data, 88% on real surveys, and discovery of four new HBSs.

Conclusion: The method is effective, reduces computational cost, and can be extended to other variable stars.

Abstract: Since the variety of their light curve morphologies, the vast majority of the
known heartbeat stars (HBSs) have been discovered by manual inspection. Machine
learning, which has already been successfully applied to the classification of
variable stars based on light curves, offers another possibility for the
automatic detection of HBSs. We propose a novel feature extraction approach for
HBSs. First, the light curve is transformed into the frequency domain via
Fourier transform, then the amplitudes of the first 100 harmonics are
extracted, and finally these harmonics are normalised as feature vectors of the
light curve. A training data set of synthetic light curves is constructed using
ELLC, and their features are fed into recurrent neural networks (RNNs) for
supervised learning, with the expected output being the eccentricity of these
light curves. The performance of RNNs is evaluated using a test data set of
synthetic light curves, achieving 95$\%$ accuracy. When applied to known HBSs
from OGLE, Kepler, and TESS surveys, the networks achieve an average accuracy
of 88$\%$. This method successfully identify four new HBSs within the eclipsing
binary catalog of Kirk et al. The use of orbital harmonics as features for HBSs
proves to be an effective approach that significantly reduces the computational
cost of neural networks. RNNs show excellent performance in recognising this
type of time series data. This method not only allows efficient identification
of HBSs, but can also be extended to recognise other types of periodic variable
stars.

</details>


### [11] [Probabilistic Zeeman-Doppler imaging of stellar magnetic fields: I. Analysis of tau Scorpii in the weak-field limit](https://arxiv.org/abs/2505.04437)
*Jennifer Rosina Andersson,Oleg Kochukhov,Zheng Zhao,Jens Sjölund*

Main category: astro-ph.SR

TL;DR: The paper proposes a Bayesian framework for probabilistic Zeeman-Doppler imaging (ZDI) to address the lack of uncertainty quantification in classical ZDI, demonstrated on the star tau Sco.


<details>
  <summary>Details</summary>
Motivation: Classical ZDI lacks reliable uncertainty quantification for magnetic field maps, limiting its reliability.

Method: A Bayesian framework with three statistical models is applied to archival ESPaDOnS data, using spherical-harmonic expansion for magnetic field parameterization.

Result: The mean magnetic field maps align with prior point estimates, but uncertainty analysis reveals higher energy content at low angular degrees.

Conclusion: The framework successfully provides uncertainty quantification in the weak-field regime, with potential for extension to more complex scenarios.

Abstract: Zeeman-Doppler imaging (ZDI) is used to study the surface magnetic field
topology of stars, based on high-resolution spectropolarimetric time series
observations. Multiple ZDI inversions have been conducted for the early B-type
star tau Sco, which has been found to exhibit a weak but complex non-dipolar
surface magnetic field. The classical ZDI framework suffers from a significant
limitation in that it provides little to no reliable uncertainty quantification
for the reconstructed magnetic field maps, with essentially all published
results being confined to point estimates. To fill this gap, we propose a
Bayesian framework for probabilistic ZDI. Here, the proposed framework is
demonstrated on tau Sco in the weak-field limit. We propose three distinct
statistical models, and use archival ESPaDOnS high-resolution Stokes V
observations to carry out the probabilistic magnetic inversion in closed form.
The surface magnetic field is parameterised by a high-dimensional
spherical-harmonic expansion. By comparing three different prior distributions
over the latent variables in the spherical-harmonic decomposition, our results
showcase the ZDI sensitivity to various hyperparameters. The mean magnetic
field maps are qualitatively similar to previously published point estimates,
but analysis of the magnetic energy distribution indicates high uncertainty and
higher energy content at low angular degrees l. Our results effectively
demonstrate that, for stars in the weak-field regime, reliable uncertainty
quantification of recovered magnetic field maps can be obtained in closed form
with natural assumptions on the statistical model. Future work will explore
extending this framework beyond the weak-field approximation and incorporating
prior uncertainty over multiple stellar parameters in more complex magnetic
inversion problems.

</details>
